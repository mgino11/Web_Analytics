{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Group 1\n",
    "# Document Classification\n",
    "Adam Gersowitz, Diego Correa, Maria A Ginorio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents. A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.\n",
    "\n",
    "Here is one example of such data: UCI Machine Learning Repository: Spambase Data Set.\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data processing packages\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# # sklearn packages\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import SGDClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Label = The last column of 'spambase.data' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.\n",
    "\n",
    "features = Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.\n",
    "\n",
    "   * 48 continuous real [0,100] attributes of type word_freq_WORD = percentage of words in the e-mail that match WORD\n",
    "    * 6 continuous real [0,100] attributes of type char_freq_CHAR = percentage of characters in the e-mail that match CHAR\n",
    "    * 1 continuous real [1,...] attribute of type capital_run_length_average = average length of uninterrupted sequences of capital letters\n",
    "    * 1 nominal {0,1} class attribute of type spam = denotes whether the e-mail was considered spam (1) or not (0),\n",
    "    i.e. unsolicited commercial e-mail."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                  0         1\nword_freq_make                0.000     0.210\nword_freq_address             0.640     0.280\nword_freq_all                 0.640     0.500\nword_freq_3d                  0.000     0.000\nword_freq_our                 0.320     0.140\nword_freq_over                0.000     0.280\nword_freq_remove              0.000     0.210\nword_freq_internet            0.000     0.070\nword_freq_order               0.000     0.000\nword_freq_mail                0.000     0.940\nword_freq_receive             0.000     0.210\nword_freq_will                0.640     0.790\nword_freq_people              0.000     0.650\nword_freq_report              0.000     0.210\nword_freq_addresses           0.000     0.140\nword_freq_free                0.320     0.140\nword_freq_business            0.000     0.070\nword_freq_email               1.290     0.280\nword_freq_you                 1.930     3.470\nword_freq_credit              0.000     0.000\nword_freq_your                0.960     1.590\nword_freq_font                0.000     0.000\nword_freq_000                 0.000     0.430\nword_freq_money               0.000     0.430\nword_freq_hp                  0.000     0.000\nword_freq_hpl                 0.000     0.000\nword_freq_george              0.000     0.000\nword_freq_650                 0.000     0.000\nword_freq_lab                 0.000     0.000\nword_freq_labs                0.000     0.000\nword_freq_telnet              0.000     0.000\nword_freq_857                 0.000     0.000\nword_freq_data                0.000     0.000\nword_freq_415                 0.000     0.000\nword_freq_85                  0.000     0.000\nword_freq_technology          0.000     0.000\nword_freq_1999                0.000     0.070\nword_freq_parts               0.000     0.000\nword_freq_pm                  0.000     0.000\nword_freq_direct              0.000     0.000\nword_freq_cs                  0.000     0.000\nword_freq_meeting             0.000     0.000\nword_freq_original            0.000     0.000\nword_freq_project             0.000     0.000\nword_freq_re                  0.000     0.000\nword_freq_edu                 0.000     0.000\nword_freq_table               0.000     0.000\nword_freq_conference          0.000     0.000\nchar_freq_;                   0.000     0.000\nchar_freq_(                   0.000     0.132\nchar_freq_[                   0.000     0.000\nchar_freq_!                   0.778     0.372\nchar_freq_$                   0.000     0.180\nchar_freq_#                   0.000     0.048\ncapital_run_length_average    3.756     5.114\ncapital_run_length_longest   61.000   101.000\ncapital_run_length_total    278.000  1028.000\nspam                          1.000     1.000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>word_freq_make</th>\n      <td>0.000</td>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>word_freq_address</th>\n      <td>0.640</td>\n      <td>0.280</td>\n    </tr>\n    <tr>\n      <th>word_freq_all</th>\n      <td>0.640</td>\n      <td>0.500</td>\n    </tr>\n    <tr>\n      <th>word_freq_3d</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_our</th>\n      <td>0.320</td>\n      <td>0.140</td>\n    </tr>\n    <tr>\n      <th>word_freq_over</th>\n      <td>0.000</td>\n      <td>0.280</td>\n    </tr>\n    <tr>\n      <th>word_freq_remove</th>\n      <td>0.000</td>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>word_freq_internet</th>\n      <td>0.000</td>\n      <td>0.070</td>\n    </tr>\n    <tr>\n      <th>word_freq_order</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_mail</th>\n      <td>0.000</td>\n      <td>0.940</td>\n    </tr>\n    <tr>\n      <th>word_freq_receive</th>\n      <td>0.000</td>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>word_freq_will</th>\n      <td>0.640</td>\n      <td>0.790</td>\n    </tr>\n    <tr>\n      <th>word_freq_people</th>\n      <td>0.000</td>\n      <td>0.650</td>\n    </tr>\n    <tr>\n      <th>word_freq_report</th>\n      <td>0.000</td>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>word_freq_addresses</th>\n      <td>0.000</td>\n      <td>0.140</td>\n    </tr>\n    <tr>\n      <th>word_freq_free</th>\n      <td>0.320</td>\n      <td>0.140</td>\n    </tr>\n    <tr>\n      <th>word_freq_business</th>\n      <td>0.000</td>\n      <td>0.070</td>\n    </tr>\n    <tr>\n      <th>word_freq_email</th>\n      <td>1.290</td>\n      <td>0.280</td>\n    </tr>\n    <tr>\n      <th>word_freq_you</th>\n      <td>1.930</td>\n      <td>3.470</td>\n    </tr>\n    <tr>\n      <th>word_freq_credit</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_your</th>\n      <td>0.960</td>\n      <td>1.590</td>\n    </tr>\n    <tr>\n      <th>word_freq_font</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_000</th>\n      <td>0.000</td>\n      <td>0.430</td>\n    </tr>\n    <tr>\n      <th>word_freq_money</th>\n      <td>0.000</td>\n      <td>0.430</td>\n    </tr>\n    <tr>\n      <th>word_freq_hp</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_hpl</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_george</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_650</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_lab</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_labs</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_telnet</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_857</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_data</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_415</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_85</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_technology</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_1999</th>\n      <td>0.000</td>\n      <td>0.070</td>\n    </tr>\n    <tr>\n      <th>word_freq_parts</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_pm</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_direct</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_cs</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_meeting</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_original</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_project</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_re</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_edu</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_table</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>word_freq_conference</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>char_freq_;</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>char_freq_(</th>\n      <td>0.000</td>\n      <td>0.132</td>\n    </tr>\n    <tr>\n      <th>char_freq_[</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>char_freq_!</th>\n      <td>0.778</td>\n      <td>0.372</td>\n    </tr>\n    <tr>\n      <th>char_freq_$</th>\n      <td>0.000</td>\n      <td>0.180</td>\n    </tr>\n    <tr>\n      <th>char_freq_#</th>\n      <td>0.000</td>\n      <td>0.048</td>\n    </tr>\n    <tr>\n      <th>capital_run_length_average</th>\n      <td>3.756</td>\n      <td>5.114</td>\n    </tr>\n    <tr>\n      <th>capital_run_length_longest</th>\n      <td>61.000</td>\n      <td>101.000</td>\n    </tr>\n    <tr>\n      <th>capital_run_length_total</th>\n      <td>278.000</td>\n      <td>1028.000</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/mgino11/Web_Analytics/main/Datasets/spambase.csv')\n",
    "\n",
    "# preview data\n",
    "df.head(2).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pandas Profiling**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "155b7a3589ff4fdcbfea15c1b07b6ae7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c684ab5c80453998232a369272fc06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2ed049f66b34a2ab15dcd319eaffc2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize\n",
    "Tokens are broken pieces of the original text that are produced after tokenization. Tokens are the basic building blocks of text -everything that helps us understand the meaning of the text is derived from tokens and the relationship to one another. For example, the character is a token in a word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "df_data['email'] = df_data.apply(lambda row: nltk.word_tokenize(row['email']), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "['Go',\n 'until',\n 'jurong',\n 'point',\n ',',\n 'crazy',\n '..',\n 'Available',\n 'only',\n 'in',\n 'bugis',\n 'n',\n 'great',\n 'world',\n 'la',\n 'e',\n 'buffet',\n '...',\n 'Cine',\n 'there',\n 'got',\n 'amore',\n 'wat',\n '...']"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[\"email\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "df_data['email'] = df_data['email'].apply(lambda x: ' '.join(\n",
    "    [word for word in x if word not in (stop_words)]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatization\n",
    "\n",
    "While stemming is just concern with giving you the stem word irrespective of its meaning, whereas lemmatization will give you a word that makes sense. For example:\n",
    "In stemming, history, historical will have the stem word as histori\n",
    "In lemmatization, the stem word will be history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Train & Test\n",
    "\n",
    "From there, we used the train_test_split function from sklearn to split our data 80/20 for training and testing purposes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_data['label'],\n",
    "                                                    df_data['email'],\n",
    "                                                    random_state=0,\n",
    "                                                    test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Features\n",
    "We further prepared our data by applying a term frequency–inverse document frequency (TFIDF) vectorizer to our email values. The TfidfVectorizer function extracts important features from our corpus.\n",
    "\n",
    "This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction.\n",
    "\n",
    "We will Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using word n-grams from n=1 to n=3 (unigrams, bigrams, and trigrams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-366c71e5abfb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mvect\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTfidfVectorizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmin_df\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mX_train_vector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dt620_36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1821\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_params\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1822\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_warn_for_unused_params\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1823\u001B[1;33m         \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1824\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tfidf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1825\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dt620_36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mfit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1201\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1202\u001B[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001B[1;32m-> 1203\u001B[1;33m                                           self.fixed_vocabulary_)\n\u001B[0m\u001B[0;32m   1204\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1205\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dt620_36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1113\u001B[0m             \u001B[0mfeature_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1114\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[1;32min\u001B[0m \u001B[0manalyze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1115\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m                     \u001B[0mfeature_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dt620_36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    102\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    103\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mpreprocessor\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 104\u001B[1;33m             \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreprocessor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    105\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m             \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dt620_36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_preprocess\u001B[1;34m(doc, accent_function, lower)\u001B[0m\n\u001B[0;32m     67\u001B[0m     \"\"\"\n\u001B[0;32m     68\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlower\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdoc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0maccent_function\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maccent_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vector = vect.transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ADD Features\n",
    "\n",
    "we can add features such as the number of digits, the dollar sign , the length of the subject line and the number of characters (anything other than a letter, digit or underscore) . This will be helpful given that usually spam emails have digits, dollar signs and lengthy subject lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "\n",
    "# Train Data\n",
    "add_length=X_train.str.len()\n",
    "add_digits=X_train.str.count(r'\\d')\n",
    "add_dollars=X_train.str.count(r'\\$')\n",
    "add_characters=X_train.str.count(r'\\W')\n",
    "\n",
    "X_train_transformed = add_feature(X_train_vector , [add_length, add_digits,  add_dollars, add_characters])\n",
    "\n",
    "# Test Data\n",
    "add_length_t=X_test.str.len()\n",
    "add_digits_t=X_test.str.count(r'\\d')\n",
    "add_dollars_t=X_test.str.count(r'\\$')\n",
    "add_characters_t=X_test.str.count(r'\\W')\n",
    "\n",
    "\n",
    "X_test_transformed = add_feature(vect.transform(X_test), [add_length_t, add_digits_t,  add_dollars_t, add_characters_t])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "## Logistic Regression\n",
    "Train the Logistic Regression Model\n",
    "We will build the Logistic Regression Model and we will report the AUC score on the test dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_train_transformed.toarray()\n",
    "# X_test_transformed.toarray()\n",
    "# np.array(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logReg_model = LogisticRegression(C=100, solver='lbfgs', multi_class='ovr', max_iter=1000)\n",
    "\n",
    "logReg_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_predicted = logReg_model.predict(X_test_transformed)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_predicted)\n",
    "auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}